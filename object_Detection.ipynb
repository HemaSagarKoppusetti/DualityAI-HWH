{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemaSagarKoppusetti/DualityAI-HWH/blob/main/object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download the dataset\n",
        "!wget -O hackathon2_train_1.zip \"https://storage.googleapis.com/duality-public-share/Datasets/hackathon2_train_1.zip\"\n",
        "\n",
        "# Step 2: Unzip the dataset\n",
        "!unzip -q hackathon2_train_1.zip -d hackathon2_dataset\n",
        "\n",
        "# Step 3: Check extracted files\n",
        "import os\n",
        "\n",
        "base_path = \"hackathon2_dataset\"\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    print(f\"ðŸ“‚ {root} - {len(files)} files\")\n",
        "    # Show first 5 files in each folder\n",
        "    print(files[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7IWDS-hFFyR",
        "outputId": "683c0a15-3977-4e67-9911-0571ec1263fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 07:42:25--  https://storage.googleapis.com/duality-public-share/Datasets/hackathon2_train_1.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.128.207, 74.125.143.207, 173.194.69.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.128.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6362208415 (5.9G) [application/zip]\n",
            "Saving to: â€˜hackathon2_train_1.zipâ€™\n",
            "\n",
            "hackathon2_train_1. 100%[===================>]   5.92G  32.5MB/s    in 2m 42s  \n",
            "\n",
            "2025-09-22 07:45:07 (37.4 MB/s) - â€˜hackathon2_train_1.zipâ€™ saved [6362208415/6362208415]\n",
            "\n",
            "ðŸ“‚ hackathon2_dataset - 0 files\n",
            "[]\n",
            "ðŸ“‚ hackathon2_dataset/train_1 - 0 files\n",
            "[]\n",
            "ðŸ“‚ hackathon2_dataset/train_1/val1 - 0 files\n",
            "[]\n",
            "ðŸ“‚ hackathon2_dataset/train_1/val1/labels - 336 files\n",
            "['000000131_vlight_uncluttered.txt', '000000052_vlight_uncluttered.txt', '000000272_vcluttered_room.txt', '000000178_vlight_cluttered_room.txt', '000000115_vcluttered_room.txt']\n",
            "ðŸ“‚ hackathon2_dataset/train_1/val1/images - 336 files\n",
            "['000000083_vlight_uncluttered.png', '000000262_vcluttered_room.png', '000000144_vcluttered_room.png', '000000236_vlight_uncluttered.png', '000000035_vcluttered_room.png']\n",
            "ðŸ“‚ hackathon2_dataset/train_1/train1 - 0 files\n",
            "[]\n",
            "ðŸ“‚ hackathon2_dataset/train_1/train1/labels - 1767 files\n",
            "['000001071_light_uncluttered.txt', '000000288_cluttered_room.txt', '000000219_cluttered_hallway.txt', '000001444_light_cluttered.txt', '000001182_cluttered_room.txt']\n",
            "ðŸ“‚ hackathon2_dataset/train_1/train1/images - 1767 files\n",
            "['000001146_light_cluttered_room.png', '000000671_light_uncluttered.png', '000001181_light_uncluttered.png', '000000634_cluttered_room.png', '000001624_light_uncluttered.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/Hackathon2_scripts.zip\" -d \"/content/Hackathon2_scripts_extracted\""
      ],
      "metadata": {
        "id": "o_EEwohBF83X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Google Colab:\n",
        "\n",
        "# Step 1: Download the zip file\n",
        "!wget -O hackathon2_test1.zip \"https://storage.googleapis.com/duality-public-share/Datasets/Hackathon2_test1.zip\"\n",
        "\n",
        "# Step 2: Unzip the downloaded file into a folder\n",
        "!unzip -q hackathon2_test1.zip -d hackathon2_test1_dataset\n",
        "\n",
        "# Step 3: Verify contents\n",
        "import os\n",
        "\n",
        "base_path = \"hackathon2_test1_dataset\"\n",
        "count = 0\n",
        "for root, dirs, files in os.walk(base_path):\n",
        "    print(f\"Folder: {root} â†’ {len(files)} files\")\n",
        "    if len(files) > 0:\n",
        "        print(\"Sample files:\", files[:5])\n",
        "    count += 1\n",
        "    # optionally stop after first few for speed\n",
        "    if count >= 3:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM3v-lAL78yA",
        "outputId": "0f2314fa-d13b-4090-f8a6-021b7dbdf3fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-22 07:46:18--  https://storage.googleapis.com/duality-public-share/Datasets/Hackathon2_test1.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.218.207, 142.251.31.207, 142.251.18.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.218.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4206767161 (3.9G) [application/zip]\n",
            "Saving to: â€˜hackathon2_test1.zipâ€™\n",
            "\n",
            "hackathon2_test1.zi 100%[===================>]   3.92G  39.6MB/s    in 1m 49s  \n",
            "\n",
            "2025-09-22 07:48:07 (37.0 MB/s) - â€˜hackathon2_test1.zipâ€™ saved [4206767161/4206767161]\n",
            "\n",
            "Folder: hackathon2_test1_dataset â†’ 0 files\n",
            "Folder: hackathon2_test1_dataset/test1 â†’ 0 files\n",
            "Folder: hackathon2_test1_dataset/test1/labels â†’ 1408 files\n",
            "Sample files: ['000000265_dark_clutter.txt', '000000125_vlight_unclutter.txt', '000000137_dark_clutter.txt', '000000896_dark_clutter.txt', '000000555_light_unclutter.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "\n",
        "# Function to predict and save images\n",
        "def predict_and_save(model, image_path, output_path, output_path_txt):\n",
        "    # Perform prediction\n",
        "    results = model.predict(image_path,conf=0.5)\n",
        "\n",
        "    result = results[0]\n",
        "    # Draw boxes on the image\n",
        "    img = result.plot()  # Plots the predictions directly on the image\n",
        "\n",
        "    # Save the result\n",
        "    cv2.imwrite(str(output_path), img)\n",
        "    # Save the bounding box data\n",
        "    with open(output_path_txt, 'w') as f:\n",
        "        for box in result.boxes:\n",
        "            # Extract the class id and bounding box coordinates\n",
        "            cls_id = int(box.cls)\n",
        "            x_center, y_center, width, height = box.xywhn[0].tolist()\n",
        "\n",
        "            # Write bbox information in the format [class_id, x_center, y_center, width, height]\n",
        "            f.write(f\"{cls_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    this_dir = Path('.')\n",
        "    os.chdir(this_dir)\n",
        "    with open(this_dir / '/content/Hackathon2_scripts_extracted/Hackathon2_scripts/yolo_params.yaml', 'r') as file:\n",
        "        data = yaml.safe_load(file)\n",
        "        if 'test' in data and data['test'] is not None:\n",
        "            images_dir = Path(data['test']) / 'images'\n",
        "        else:\n",
        "            print(\"No test field found in yolo_params.yaml, please add the test field with the path to the test images\")\n",
        "            exit()\n",
        "\n",
        "    # check that the images directory exists\n",
        "    if not images_dir.exists():\n",
        "        print(f\"Images directory {images_dir} does not exist\")\n",
        "        exit()\n",
        "\n",
        "    if not images_dir.is_dir():\n",
        "        print(f\"Images directory {images_dir} is not a directory\")\n",
        "        exit()\n",
        "\n",
        "    if not any(images_dir.iterdir()):\n",
        "        print(f\"Images directory {images_dir} is empty\")\n",
        "        exit()\n",
        "\n",
        "    # Load the YOLO model\n",
        "    detect_path = this_dir / \"runs\" / \"detect\"\n",
        "    # Added check for directory existence\n",
        "    if not detect_path.exists():\n",
        "        print(f\"Error: The directory '{detect_path}' was not found.\")\n",
        "        print(\"Please ensure that the model training cell (cell 77689678) has been executed successfully before running this cell.\")\n",
        "        exit()\n",
        "\n",
        "    train_folders = [f for f in os.listdir(detect_path) if os.path.isdir(detect_path / f) and f.startswith(\"train\")]\n",
        "    if len(train_folders) == 0:\n",
        "        raise ValueError(\"No training folders found\")\n",
        "    idx = 0\n",
        "    if len(train_folders) > 1:\n",
        "        choice = -1\n",
        "        choices = list(range(len(train_folders)))\n",
        "        while choice not in choices:\n",
        "            print(\"Select the training folder:\")\n",
        "            for i, folder in enumerate(train_folders):\n",
        "                print(f\"{i}: {folder}\")\n",
        "            choice = input()\n",
        "            if not choice.isdigit():\n",
        "                choice = -1\n",
        "            else:\n",
        "                choice = int(choice)\n",
        "        idx = choice\n",
        "\n",
        "    model_path = detect_path / train_folders[idx] / \"weights\" / \"best.pt\"\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Directory with images\n",
        "    output_dir = this_dir / \"predictions\" # Replace with the directory where you want to save predictions\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create images and labels subdirectories\n",
        "    images_output_dir = output_dir / 'images'\n",
        "    labels_output_dir = output_dir / 'labels'\n",
        "    images_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    labels_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Iterate through the images in the directory\n",
        "    for img_path in images_dir.glob('*'):\n",
        "        if img_path.suffix not in ['.png', '.jpg']:\n",
        "            continue\n",
        "        output_path_img = images_output_dir / img_path.name  # Save image in 'images' folder\n",
        "        output_path_txt = labels_output_dir / img_path.with_suffix('.txt').name  # Save label in 'labels' folder\n",
        "        predict_and_save(model, img_path, output_path_img, output_path_txt)\n",
        "\n",
        "    print(f\"Predicted images saved in {images_output_dir}\")\n",
        "    print(f\"Bounding box labels saved in {labels_output_dir}\")\n",
        "    data = this_dir / 'yolo_params.yaml'\n",
        "    print(f\"Model parameters saved in {data}\")\n",
        "    metrics = model.val(data=data, split=\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "RZBd-bMP9Llq",
        "outputId": "0c3f5abc-bd3a-46a4-9f05-3255fc758aad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.202)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cpu)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.33.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'runs/detect'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2151591491.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Load the YOLO model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdetect_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"runs\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"detect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtrain_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No training folders found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/detect'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77689678",
        "outputId": "40e72a42-ca39-4fac-9fa7-9aea94fdd56e"
      },
      "source": [
        "# Train the model\n",
        "model = YOLO('yolov8n.pt')  # Load a pretrained model\n",
        "results = model.train(data='/content/Hackathon2_scripts_extracted/Hackathon2_scripts/yolo_params.yaml', epochs=10)  # Train the model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 101.8MB/s 0.1s\n",
            "Ultralytics 8.3.202 ðŸš€ Python-3.12.11 torch-2.8.0+cpu CPU (AMD EPYC 7B13)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Hackathon2_scripts_extracted/Hackathon2_scripts/yolo_params.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 19.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=7\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5207.5Â±3478.5 MB/s, size: 3003.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/hackathon2_dataset/train_1/train1/labels... 1767 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1767/1767 478.7it/s 3.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/hackathon2_dataset/train_1/train1/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 7642.6Â±4189.7 MB/s, size: 2819.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/hackathon2_dataset/train_1/val1/labels... 336 images, 13 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 336/336 524.0it/s 0.6s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/hackathon2_dataset/train_1/val1/labels.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10         0G      1.037      3.331      1.115         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:54\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 22.1s\n",
            "                   all        336       1005      0.515      0.278      0.275      0.193\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10         0G      1.031      2.275        1.1         42        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 21.4s\n",
            "                   all        336       1005      0.689      0.374      0.432      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10         0G       1.05      1.927      1.094         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.9s\n",
            "                   all        336       1005      0.609      0.394      0.442      0.307\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10         0G      0.984       1.65      1.062         14        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:49\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.7s\n",
            "                   all        336       1005      0.764       0.43      0.519      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10         0G     0.9198      1.408      1.028         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.4s\n",
            "                   all        336       1005      0.725      0.486      0.553      0.399\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10         0G     0.8777      1.253      1.006          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.7s\n",
            "                   all        336       1005       0.72      0.533      0.586      0.424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10         0G     0.8275      1.119     0.9875         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:50\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.3s\n",
            "                   all        336       1005      0.777      0.556       0.62       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10         0G     0.7613      1.011     0.9531         48        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:49\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.3s\n",
            "                   all        336       1005      0.797      0.555      0.637      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10         0G     0.7077     0.9077     0.9326         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:49\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.3s\n",
            "                   all        336       1005      0.824      0.545      0.637      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10         0G     0.6881      0.855     0.9217         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 111/111 0.5it/s 3:49\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.5it/s 20.3s\n",
            "                   all        336       1005      0.859      0.582       0.67      0.523\n",
            "\n",
            "10 epochs completed in 0.697 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.202 ðŸš€ Python-3.12.11 torch-2.8.0+cpu CPU (AMD EPYC 7B13)\n",
            "Model summary (fused): 72 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 11/11 0.6it/s 19.8s\n",
            "                   all        336       1005      0.858      0.582       0.67      0.523\n",
            "            OxygenTank        156        257      0.873      0.658      0.754      0.625\n",
            "          NitrogenTank        165        300      0.824      0.683      0.754      0.647\n",
            "           FirstAidBox         82        121      0.859      0.669      0.729      0.589\n",
            "             FireAlarm         54         60      0.969      0.513      0.604      0.488\n",
            "     SafetySwitchPanel         53         58      0.848      0.552      0.642      0.428\n",
            "        EmergencyPhone         63         74      0.832      0.486      0.584      0.393\n",
            "      FireExtinguisher         94        135      0.802      0.511       0.62       0.49\n",
            "Speed: 0.2ms preprocess, 12.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "\n",
        "# Function to predict and save images\n",
        "def predict_and_save(model, image_path, output_path, output_path_txt):\n",
        "    # Perform prediction\n",
        "    results = model.predict(image_path,conf=0.5)\n",
        "\n",
        "    result = results[0]\n",
        "    # Draw boxes on the image\n",
        "    img = result.plot()  # Plots the predictions directly on the image\n",
        "\n",
        "    # Save the result\n",
        "    cv2.imwrite(str(output_path), img)\n",
        "    # Save the bounding box data\n",
        "    with open(output_path_txt, 'w') as f:\n",
        "        for box in result.boxes:\n",
        "            # Extract the class id and bounding box coordinates\n",
        "            cls_id = int(box.cls)\n",
        "            x_center, y_center, width, height = box.xywhn[0].tolist()\n",
        "\n",
        "            # Write bbox information in the format [class_id, x_center, y_center, width, height]\n",
        "            f.write(f\"{cls_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    this_dir = Path('.')\n",
        "    os.chdir(this_dir)\n",
        "    with open(this_dir / '/content/Hackathon2_scripts_extracted/Hackathon2_scripts/yolo_params.yaml', 'r') as file:\n",
        "        data = yaml.safe_load(file)\n",
        "        if 'test' in data and data['test'] is not None:\n",
        "            images_dir = Path(data['test']) / 'images'\n",
        "        else:\n",
        "            print(\"No test field found in yolo_params.yaml, please add the test field with the path to the test images\")\n",
        "            exit()\n",
        "\n",
        "    # check that the images directory exists\n",
        "    if not images_dir.exists():\n",
        "        print(f\"Images directory {images_dir} does not exist\")\n",
        "        exit()\n",
        "\n",
        "    if not images_dir.is_dir():\n",
        "        print(f\"Images directory {images_dir} is not a directory\")\n",
        "        exit()\n",
        "\n",
        "    if not any(images_dir.iterdir()):\n",
        "        print(f\"Images directory {images_dir} is empty\")\n",
        "        exit()\n",
        "\n",
        "    # Load the YOLO model\n",
        "    detect_path = this_dir / \"runs\" / \"detect\"\n",
        "    train_folders = [f for f in os.listdir(detect_path) if os.path.isdir(detect_path / f) and f.startswith(\"train\")]\n",
        "    if len(train_folders) == 0:\n",
        "        raise ValueError(\"No training folders found\")\n",
        "    idx = 0\n",
        "    if len(train_folders) > 1:\n",
        "        choice = -1\n",
        "        choices = list(range(len(train_folders)))\n",
        "        while choice not in choices:\n",
        "            print(\"Select the training folder:\")\n",
        "            for i, folder in enumerate(train_folders):\n",
        "                print(f\"{i}: {folder}\")\n",
        "            choice = input()\n",
        "            if not choice.isdigit():\n",
        "                choice = -1\n",
        "            else:\n",
        "                choice = int(choice)\n",
        "        idx = choice\n",
        "\n",
        "    model_path = detect_path / train_folders[idx] / \"weights\" / \"best.pt\"\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Directory with images\n",
        "    output_dir = this_dir / \"predictions\" # Replace with the directory where you want to save predictions\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create images and labels subdirectories\n",
        "    images_output_dir = output_dir / 'images'\n",
        "    labels_output_dir = output_dir / 'labels'\n",
        "    images_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    labels_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Iterate through the images in the directory\n",
        "    for img_path in images_dir.glob('*'):\n",
        "        if img_path.suffix not in ['.png', '.jpg']:\n",
        "            continue\n",
        "        output_path_img = images_output_dir / img_path.name  # Save image in 'images' folder\n",
        "        output_path_txt = labels_output_dir / img_path.with_suffix('.txt').name  # Save label in 'labels' folder\n",
        "        predict_and_save(model, img_path, output_path_img, output_path_txt)\n",
        "\n",
        "    print(f\"Predicted images saved in {images_output_dir}\")\n",
        "    print(f\"Bounding box labels saved in {labels_output_dir}\")\n",
        "    data = this_dir / 'yolo_params.yaml'\n",
        "    print(f\"Model parameters saved in {data}\")\n",
        "    metrics = model.val(data=data, split=\"test\")"
      ],
      "metadata": {
        "id": "VaIUMemnIDEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}